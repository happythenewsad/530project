{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_path = './semeval2017-task8-dataset/traindev/rumoureval-subtaskB-train.json'\n",
    "dev_path = './semeval2017-task8-dataset/traindev/rumoureval-subtaskB-dev.json' \n",
    "eval_path = './semeval2017-task8-dataset/rumoureval-data/'\n",
    "\n",
    "test_folder_path = './semeval2017-task8-test-data/'\n",
    "\n",
    "def source_tweet_data(tweet_id, folder_path_dict, simple=True):\n",
    "    tweet_data = {}\n",
    "    \n",
    "    folder_path = folder_path_dict[tweet_id]\n",
    "    source_tweet_path = folder_path + 'source-tweet/' + tweet_id + '.json'\n",
    "    \n",
    "    with open(source_tweet_path, 'r') as f:\n",
    "        source_tweet_str = f.read()\n",
    "\n",
    "    source_tweet = json.loads(source_tweet_str)    \n",
    "        \n",
    "    #Take only the text data\n",
    "    #FOR BASELINE IMPLEMENTATION\n",
    "    if simple:\n",
    "        tweet_data['text'] = source_tweet['text']\n",
    "    \n",
    "    #Use all information in the source tweet \n",
    "    #NOT USED FOR BASELINE, BUT MAY BE USEFUL FOR GENERATING MORE FEATURES\n",
    "    else:\n",
    "        tweet_data = source_tweet\n",
    "    \n",
    "    #TODO: EXTRACT INFORMATION FROM TWEET REPLIES\n",
    "    \n",
    "    #does the tweet have context? (boolean value)\n",
    "    has_context = os.path.isdir(folder_path + 'context')    \n",
    "    tweet_data['has_context'] = has_context\n",
    "    \n",
    "    #if it does, point to context path\n",
    "    if has_context:\n",
    "        tweet_data['context_path'] = folder_path + 'context/'\n",
    "    else:\n",
    "        tweet_data['context_path'] = np.nan    \n",
    "    \n",
    "    return tweet_data\n",
    "\n",
    "def load_train_dev(train_path, dev_path, eval_path, simple=True):\n",
    "\n",
    "    with open(train_path, 'r') as f1, open(dev_path, 'r') as f2:\n",
    "        train_str = f1.read()\n",
    "        dev_str = f2.read()\n",
    "\n",
    "    train_dict = json.loads(train_str)    \n",
    "    dev_dict = json.loads(dev_str)\n",
    "    train_data = {}\n",
    "    dev_data = {}\n",
    "\n",
    "    folder_path_dict = {}\n",
    "    \n",
    "    topic_list = os.listdir(eval_path)\n",
    "    topic_dict = {}\n",
    "\n",
    "    #maintain folder path dictionary to use during feature generation\n",
    "    for topic in topic_list:\n",
    "\n",
    "        for tweet_id in os.listdir(eval_path + topic):\n",
    "\n",
    "            #keep track of topic-id pairs \n",
    "            topic_dict[tweet_id] = topic\n",
    "\n",
    "            #add id-folderpath pairs\n",
    "            folder_path_dict[tweet_id] = eval_path + topic + '/' + tweet_id + '/'\n",
    "            \n",
    "      \n",
    "    #generate features for training data\n",
    "    for tweet_id in train_dict.keys():\n",
    "        train_data[tweet_id] = source_tweet_data(tweet_id, folder_path_dict, simple)\n",
    "        \n",
    "        #note that the test data does not have explicit topic labels and therefore must have a separate process to extract topic from them\n",
    "        train_data[tweet_id]['topic'] = topic_dict[tweet_id]\n",
    "        train_data[tweet_id]['classification'] = train_dict[tweet_id]\n",
    "\n",
    "    #generate features for dev data\n",
    "    for tweet_id in dev_dict.keys():\n",
    "        dev_data[tweet_id] = source_tweet_data(tweet_id, folder_path_dict, simple)\n",
    "        dev_data[tweet_id]['topic'] = dev_dict[tweet_id]\n",
    "        dev_data[tweet_id]['classification'] = dev_dict[tweet_id]        \n",
    "    \n",
    "    #save as pandas dataframe\n",
    "    train_df = pd.DataFrame(train_data).transpose()\n",
    "    dev_df = pd.DataFrame(dev_data).transpose()\n",
    "    \n",
    "    return train_data, dev_data, train_df, dev_df\n",
    "\n",
    "def load_test_data(test_folder_path, simple=True):\n",
    "    \n",
    "    test_data = {}    \n",
    "    folder_path_dict = {}\n",
    "\n",
    "    #maintain folder path dictionary to use during feature generation\n",
    "    for tweet_id in os.listdir(test_folder_path):\n",
    "        #add id-folderpath pairs\n",
    "        folder_path_dict[tweet_id] = test_folder_path + '/' + tweet_id + '/'\n",
    "    \n",
    "    #generate features for test data\n",
    "    for tweet_id in os.listdir(test_folder_path):\n",
    "        test_data[tweet_id] = source_tweet_data(tweet_id, folder_path_dict, simple)\n",
    "        \n",
    "    \n",
    "    #save as pandas dataframe\n",
    "    test_df = pd.DataFrame(test_data).transpose()\n",
    "        \n",
    "    return test_data, test_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print('generating features..')\n",
    "    train_data_simple, dev_data_simple, train_df_simple, dev_df_simple = load_train_dev(train_path, dev_path, eval_path, simple=True)\n",
    "    test_data_simple, test_df_simple = load_test_data(test_folder_path, simple=True)\n",
    "\n",
    "    train_data_full, dev_data_full, train_df_full, dev_df_full = load_train_dev(train_path, dev_path, eval_path, simple=False)\n",
    "    test_data_full, test_df_full = load_test_data(test_folder_path, simple=False)\n",
    "    \n",
    "    data_list = [train_data_simple, dev_data_simple, test_data_simple, train_data_full, dev_data_full, test_data_full]\n",
    "    df_list = [train_df_simple, dev_df_simple, test_df_simple, train_df_full, dev_df_full, test_df_full]\n",
    "\n",
    "    data_name_list = ['train_data_simple', 'dev_data_simple', 'test_data_simple', 'train_data_full', 'dev_data_full', 'test_data_full']\n",
    "\n",
    "    \n",
    "    print('saving data to output..')\n",
    "    \n",
    "    #crete output folder\n",
    "    output_folder_path = './output'\n",
    "    simple_path = output_folder_path + '/simple'\n",
    "    full_path = output_folder_path + '/full'\n",
    "    folder_list = [output_folder_path, simple_path, full_path]\n",
    "    for folder in folder_list:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "    \n",
    "    #save data\n",
    "    for idx in range(len(data_list)):\n",
    "        \n",
    "        data_name = data_name_list[idx]\n",
    "        if '_simple' in data_name:        \n",
    "            json_output_name = simple_path + '/' + data_name_list[idx] + '.json'\n",
    "            pickle_output_name = simple_path + '/' + data_name_list[idx] + '.pickle'\n",
    "        else:        \n",
    "            json_output_name = full_path + '/' + data_name_list[idx] + '.json'\n",
    "            pickle_output_name = full_path + '/' + data_name_list[idx] + '.pickle'\n",
    "            \n",
    "        with open(json_output_name, 'w') as f:\n",
    "            f.write(json.dumps(data_list[idx]))\n",
    "            \n",
    "        df_list[idx].to_pickle(pickle_output_name)\n",
    "        \n",
    "\n",
    "    print('test code: sample of dev data (full version) \\n')    \n",
    "    #TEST CODE FOR READING PANDAS DATAFRAME\n",
    "    df = pd.read_pickle('./output/full/dev_data_full.pickle')\n",
    "    print(df.head())\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    #TEST CODE FOR READING JSON AS DICT\n",
    "    with open('./output/full/dev_data_full.json', 'r') as f:\n",
    "        jstr = f.read()\n",
    "\n",
    "    j = json.loads(jstr)\n",
    "    \n",
    "    print(next(iter(j.values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
