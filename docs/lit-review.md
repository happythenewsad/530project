__Literature Review__

&nbsp; The only team that had participated in subtask B-closed was IITP(Singh et al., 2017). The team uses the Naive Bayes classifier (which they reported to yield the best performance) after generating features from the text contents of the tweet. The text is preprocessed to normalize all urls and user references. The battery of text features include word embedding, vulgar words, presence of metadata, word count, POS tag, negation words, wh- words, opinion words as obtained from MPQA subjectivity lexicon, and the number of adjectives.    
      
&nbsp; IITP qualified for the 'open' subtask by including a binary variable that indicated whether the tweet had media context to support its claims. It should be noted that the team had the lowest score among the 5 participants for the ‘closed’ subtask at 0.286 and their ‘open’ model performed marginally better, with the score of 0.393. This leaves open much room for improvement, especially since the tweets’ source material and the reply chain had not been explored.   
      
&nbsp; To gain insight into task B, we reviewed work by teams IKM(Chen et al., 2017) and NileTMRG(Enayet and El-Beltagy, 2017), who tied for first place on subtask B-open with the score of 0.536. IKM implemented a convolutional neural network, using the word embedding matrices generated from the tweets as features. The CNN is composed of a convolution layer, a pooling layer, and a final fully connected layer with a softmax activation function. The results display the potential of neural network models, which may be further investigated in our main study.      
      
&nbsp; Meanwhile, NileTMRG took an approach similar to IITP, where they used a linear SVM model on the following text features: question existence, denial terms, support words, hashtags, urls, whether the tweet is a reply, tweet sentiment. In addition, they also utilized the tweet’s metadata, such as user verification, number of friends/followers/past tweets, retweet ratio, existence of profile photo, days since user creation, and whether the source tweet was verified. The addition of metadata features contributed in improving the performance of the model by a significant amount.      
             
&nbsp; In our research, we first replicate the IITP model as our baseline for Rumoreval subtask B-open. We then improve the baseline by adding metadata features used by NileTMRG and the context features. We expect to have a significant boost in performance by fully utilizing the information gained from the context data, which contain webpages and wikipedia entries. Lastly, we explore the CNN model proposed by IKM with the inclusion of the context features.    

