{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses basic classifiers (naive bayes, svm, decision tree, random forest) to predict labels for rumoreval, using binary and scalar features, along with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, \"./code\")\n",
    "from FileReader import FileReader\n",
    "sys.path.insert(1, \"./code/feature-extraction/embed-extractor\")\n",
    "from EmbedExtractor import EmbedExtractor\n",
    "sys.path.insert(1, \"./code/feature-extraction/vulgar-extractor\")\n",
    "from VulgarExtractor import VulgarExtractor\n",
    "sys.path.insert(1, \"./code/feature-extraction/twitter-parser\")\n",
    "from TwitterParser import TwitterParser\n",
    "sys.path.insert(1, \"./code/feature-extraction/opinion-extractor/\")\n",
    "from OpinionExtractor import OpinionExtractor\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import classifiers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This execution will give you the correct files to run this program. It includes all of the data of the training, dev, and goldtest sets in pd.dataframe form, as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to output..\n"
     ]
    }
   ],
   "source": [
    "classInstance = FileReader\n",
    "df_list = classInstance.get_dataframe() #IMPORTANT:  saves a pickle to output/simple or output/full. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell will generate the pre-PCA word embeddings and the post-PCA word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "26\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "26\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "27\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "26\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "27\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n",
      "UH OH\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to generate the pre-PCA and post-PCA word embeddings\n",
    "def chunkIt(seq, num):\n",
    "    print(len(seq))\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    zeros = [0]*int(avg)\n",
    "\n",
    "    while last < len(seq):\n",
    "        subVector = seq[int(last):int(last + avg)]\n",
    "        if zeros==subVector and last>4700:\n",
    "            break\n",
    "        out.append(subVector)\n",
    "        last += avg\n",
    "    return out\n",
    "\n",
    "###UNCOMMENT THIS TO RUN THE EE#####\n",
    "ee = EmbedExtractor()\n",
    "tweet2vec = {}\n",
    "    \n",
    "for d in [\"train\",\"dev\",\"test\"]:\n",
    "\twith open('./output/full/'+d+'_data_full.json', 'r') as f:\n",
    "\t    jstr = f.read()\n",
    "\n",
    "\tj = json.loads(jstr)\n",
    "\n",
    "\tfor key in j:\n",
    "\t\ttweet=j[key]['text']\n",
    "\t\ttweet2vec[key]=ee.tweetVec(tweet)\n",
    "pickleFile=\"code/feature-extraction/embed-extractor/word_embedding_vectors.pickle\"\n",
    "pickle.dump(tweet2vec,open(pickleFile,\"wb\"))\n",
    "##########################################\n",
    "\n",
    "\n",
    "word_embeddings_pca = {}\n",
    "pickleFile=\"code/feature-extraction/embed-extractor/word_embedding_vectors.pickle\"\n",
    "word_embeddings = pd.read_pickle(pickleFile)\n",
    "\n",
    "word_embeddings_chunked = {}\n",
    "for word in word_embeddings:\n",
    "    word_embeddings_chunked[word]=chunkIt(word_embeddings[word],30)\n",
    "for word in word_embeddings_chunked:\n",
    "    pca = decomposition.PCA(n_components=24)\n",
    "    x = np.array(word_embeddings_chunked[word])\n",
    "    try:\n",
    "        x_std = StandardScaler().fit_transform(x)\n",
    "        pca.fit_transform(x_std)\n",
    "        word_embeddings_pca[word]=pca.singular_values_\n",
    "#         print(len(pca.singular_values_))\n",
    "    except ValueError:\n",
    "        # print(word_embeddings_chunked[word])\n",
    "        print(\"UH OH\")\n",
    "\n",
    "pickle.dump(word_embeddings_pca,open(\"code/feature-extraction/embed-extractor/word_embedding_vectors_pca.pickle\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A list of functions for data normalization. Should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_opinion_column(df, strongly_subj_list):\n",
    "    #add a binary column where opinion == 1 if the tweet text contains a strongly subjective word\n",
    "    #global strongly_subj_list\n",
    "    OpinionExtractor.add_opinion_column(df, strongly_subj_list)\n",
    "\n",
    "def extract_user_column(row):\n",
    "    global user_labels\n",
    "        \n",
    "    user_dict = row['user']\n",
    "    \n",
    "    for col in user_dict.keys():\n",
    "        user_labels.append(col)\n",
    "\n",
    "def update_user_column(row):\n",
    "    global user_vals\n",
    "    user_dict = row['user']\n",
    "    \n",
    "    for key in user_vals.keys():\n",
    "#         print(key)\n",
    "        concat_key = key[5:]\n",
    "        if concat_key in user_dict:\n",
    "#             print(sup)\n",
    "            val = user_dict[concat_key]\n",
    "            user_vals[key].append(val)\n",
    "        else:\n",
    "            user_vals[key].append(np.nan)\n",
    "            \n",
    "def convert_date(row):\n",
    "    date = row['user_created_at'].split()\n",
    "    date_str = ' '.join([date[1], date[2], date[-1]])\n",
    "    \n",
    "    datetime_object = datetime.strptime(date_str, '%b %d %Y')\n",
    "    date_int = datetime_object.year * 10000 + datetime_object.month * 100 + datetime_object.day\n",
    "        \n",
    "    return date_int\n",
    "\n",
    "def convert_to_int(row, col):\n",
    "    return int(row[col])\n",
    "\n",
    "def normalize_column(df, col):\n",
    "    col_array = np.asarray(df[col].tolist())\n",
    "    mean = np.mean(col_array)\n",
    "    std = np.std(col_array)\n",
    "    col_array = (col_array - mean) / float(std)\n",
    "    \n",
    "    df[col] = col_array\n",
    "    \n",
    "def create_user_features(df):\n",
    "\n",
    "    global user_labels\n",
    "    global user_vals\n",
    "    user_labels = []\n",
    "    df.apply(extract_user_column, axis = 1)\n",
    "    user_labels = ['user_' + label for label in set(user_labels)]\n",
    "    user_vals = {label:[] for label in user_labels}\n",
    "    \n",
    "    df.apply(update_user_column, axis = 1) \n",
    "    \n",
    "    user_df = pd.DataFrame(user_vals)\n",
    "    user_df['user_created'] = user_df.apply(convert_date, axis = 1)\n",
    "\n",
    "    col_list = ['user_default_profile', \n",
    "                'user_favourites_count', 'user_followers_count', 'user_friends_count', 'user_geo_enabled',\n",
    "                'user_listed_count', 'user_statuses_count', 'user_verified','user_created']\n",
    "\n",
    "    for col in col_list:\n",
    "        user_df[col] = user_df.apply(lambda x : convert_to_int(x, col), axis = 1)\n",
    "\n",
    "    # normalize_column(user_df, col)\n",
    "    user_df = user_df[col_list]    \n",
    "\n",
    "    norm_list = ['user_favourites_count', 'user_followers_count', 'user_friends_count',\n",
    "                'user_listed_count', 'user_statuses_count','user_created']\n",
    "\n",
    "    for col in norm_list:\n",
    "        normalize_column(user_df, col)\n",
    "\n",
    "    df = pd.concat([df.reset_index(), user_df], axis = 1)\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now load the word embeddings. These can come from the following:\n",
    "\n",
    "    PRE-PCA EMBEDDINGS:\n",
    "     'code/feature-extraction/embed-extractor/word_embedding_vectors.pickle'\n",
    "    POST-PCA EMBEDDINGS:\n",
    "     'code/feature-extraction/embed-extractor/word_embedding_vectors_pca.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('code/feature-extraction/embed-extractor/word_embedding_vectors.pickle', 'rb') as pickle_file:\n",
    "    ee = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here, we can pick the attributes that we want to include in our model. All current attributes mentioned in cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attributes = ['isVulgar', 'containsAdjective', 'containsEmoji', 'containsURL', 'containsAbbreviation', 'wordCount']\n",
    "for tag in TwitterParser.tagset:\n",
    "    attributes.append('num_' + tag)\n",
    "\n",
    "attributes = attributes + ['num_replies', 're_has_?', 're_has_NOT', 're_has_correct',\n",
    " 're_has_credib', 're_has_data', 're_has_detail', 're_has_fabricat', 're_has_lie', 're_has_proof', \n",
    "                  're_has_source', 're_has_witness']\n",
    "\n",
    "# ['opinion', 'user_default_profile',\n",
    "#  'user_favourites_count', 'user_followers_count', 'user_friends_count', 'user_geo_enabled', 'user_listed_count', \n",
    "#  'user_statuses_count', 'user_verified', 'user_created']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here we can define functions that normalize and transform our data suitable for the classifier.\n",
    "\n",
    "## ******Note that you can toggle on/off inclusion of word embeddings through here. Please read code and look for comment that mentions word embeddings.******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(column_name, df):\n",
    "    std = df[column_name].std()\n",
    "    norm_col = df[column_name].apply(lambda x: x - std)\n",
    "    df[column_name] = norm_col\n",
    "\n",
    "# builds the labels and vectorizations of given data\n",
    "#if you want to fool around with including/excluding certain features and whatnot, this is the place to do it\n",
    "\n",
    "def labels_and_vectors(file, index=0):\n",
    "    df = pd.read_pickle(file)\n",
    "    \n",
    "    wordlist = VulgarExtractor.vulgarWords(\"code/feature-extraction/vulgar-extractor/badwords.txt\") \n",
    "    dftext = df[['text']]\n",
    "    result = dftext.applymap(lambda x: VulgarExtractor.containsVulgar(x,wordlist))\n",
    "    df['isVulgar'] = result\n",
    "\n",
    "    word_embeddings = [ee[key] for key in df.index]\n",
    "    # word_embeddings = [ee.tweetVec(tagged_line) for tagged_line in df['text']]\n",
    "    textlist = [txt.replace('\\n','') for txt in df['text'].tolist()]\n",
    "    tagged_sents = TwitterParser.tag(textlist)\n",
    "    df['POS'] = tagged_sents\n",
    "\n",
    "    processed_sents = []\n",
    "    for tagged_sent in df['POS']:\n",
    "        processed_words = []\n",
    "        for word, tag in tagged_sent:\n",
    "            if tag == 'U':\n",
    "                processed_words.append('someurl')\n",
    "            elif tag == '@':\n",
    "                processed_words.append('@someuser')\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "        sent = ' '.join(processed_words)\n",
    "        processed_sents.append(sent)\n",
    "    df['text'] = processed_sents\n",
    "\n",
    "    word_counts = [TwitterParser.word_count(tagged_line) for tagged_line in df['POS']]\n",
    "    pos_count_list = [TwitterParser.pos_counts(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_adjs = [TwitterParser.contains_adjectives(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_urls = [TwitterParser.contains_url(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_emojis = [TwitterParser.contains_emoji(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_abbrevs = [TwitterParser.contains_abbreviation(tagged_line) for tagged_line in df['POS']]\n",
    "\n",
    "    df['wordCount'] = word_counts\n",
    "    df['posCounts'] = pos_count_list\n",
    "    df['containsAdjective'] = contains_adjs\n",
    "    df['containsURL'] = contains_urls\n",
    "    df['containsEmoji'] = contains_emojis\n",
    "    df['containsAbbreviation'] = contains_abbrevs\n",
    "    df['wordEmbedding'] = word_embeddings\n",
    "\n",
    "    \n",
    "    for i, tag in enumerate(TwitterParser.tagset):\n",
    "        tag_counts = []\n",
    "        for pos_counts in df['posCounts']:\n",
    "            tag_counts.append(pos_counts[i])\n",
    "        column_name = 'num_' + tag\n",
    "        df[column_name] = tag_counts\n",
    "        normalize(column_name, df)\n",
    "    \n",
    "    # global strongly_subj_list\n",
    "    strongly_subj_list = OpinionExtractor.initialize_subjectivity()\n",
    "    create_opinion_column(df, strongly_subj_list)\n",
    "    df = create_user_features(df)    \n",
    "        \n",
    "    # Changes \"true\"/\"false\"/\"unverified\" to numeric values, just like the in the early cells\n",
    "    df.loc[df.classification == 'true', 'classification'] = 1\n",
    "    df.loc[df.classification == 'false', 'classification'] = 0\n",
    "    df.loc[df.classification == 'unverified', 'classification'] = 2\n",
    "   \n",
    "    # getting the labels     \n",
    "    labels = df['classification']\n",
    "    labels = [l for l in labels]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # getting the values as a list of lists\n",
    "    values = df[attributes].values.tolist()\n",
    "    word_embedding_values = df['wordEmbedding'].values.tolist()\n",
    "\n",
    "\n",
    "#     #Below puts the tweet ID as a feature. Comment this out if you aren't using tweetID\n",
    "#     for i,index in enumerate(df.index):\n",
    "#         dev_values[i].append(int(index))\n",
    "\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## UNCOMMENT THIS IN ORDER TO INCOPORATE WORD_EMBEDDINGS AGAIN\n",
    "    #word_embedding_values = df['wordEmbedding'].values.tolist()\n",
    "    #for i,d in enumerate(word_embedding_values):\n",
    "     #   values[i].extend(d)\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    values = np.array(values)\n",
    "    if index == 1:\n",
    "        return df.index, values\n",
    "    \n",
    "    \n",
    "    return labels, values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here, you can set where your training and target data comes from (we just \"call it\" dev_values here). Please maintain the index variable seen in the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './feature-extraction/twitter-parser/runTagger.sh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-06ccad1a9144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_and_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/full/train_data_full.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_and_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/full/goldtest_data_full.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-ab2bcaaa9a57>\u001b[0m in \u001b[0;36mlabels_and_vectors\u001b[0;34m(file, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# word_embeddings = [ee.tweetVec(tagged_line) for tagged_line in df['text']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtextlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagged_sents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m./feature-extraction/twitter-parser/TwitterParser.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(tweets)\u001b[0m\n",
      "\u001b[0;32m/Users/ncwphilly/anaconda/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 336\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ncwphilly/anaconda/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stdin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ncwphilly/anaconda/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    705\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ncwphilly/anaconda/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                 \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_executable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './feature-extraction/twitter-parser/runTagger.sh'"
     ]
    }
   ],
   "source": [
    "tr_labels, tr_values = labels_and_vectors('output/full/train_data_full.pickle')\n",
    "indices, dev_values = labels_and_vectors('output/full/goldtest_data_full.pickle', index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By executing this script, we train the model get the label predictions.\n",
    "\n",
    "## NOTE: This is where you should experiment with different classifiers.\n",
    "\n",
    "There is currently:\n",
    "\n",
    "classifiers.naive_bayes(tr_values, tr_labels, dev_values)\n",
    "\n",
    "classifiers.svm_classifier(tr_values, tr_labels, dev_values)\n",
    "\n",
    "classifiers.decision_tree_classifier(tr_values, tr_labels, dev_values)\n",
    "\n",
    "classifiers.random_forest(tr_values, tr_labels, dev_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-70a7ff28c81e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#change classifier here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# need to convert the numerical predictions back into their string values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tr_values' is not defined"
     ]
    }
   ],
   "source": [
    "#change classifier here\n",
    "predictions, probabilities = classifiers.random_forest(tr_values, tr_labels, dev_values, 80, 3, \"gini\")\n",
    "ps = []\n",
    "\n",
    "# need to convert the numerical predictions back into their string values\n",
    "for i, p in enumerate(predictions):\n",
    "    if p == 0:\n",
    "        ps.append('false')\n",
    "    if p == 1:\n",
    "        ps.append('true')\n",
    "    if p == 2:\n",
    "        ps.append('unverified')\n",
    "\n",
    "# creates pairings of the prediction and the probability of the prediction\n",
    "pred_probs_pairs = [[ps[i], probabilities[i][predictions[i]]] for i in range(len(predictions))] \n",
    "#attaches the tweetID (called reference_id in the score.py file)\n",
    "pred_dict = {index:pred_probs_pairs[i] for i,index in enumerate(indices)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to json and scoring script. The first argument is the gold set and the second argument is the predictions you should have generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = './output/classifier_output/'\n",
    "try:\n",
    "    os.stat(output_dir)\n",
    "except:\n",
    "    os.mkdir(output_dir)  \n",
    "\n",
    "with open('output/classifier_output/goldtest_nb.json', 'w') as outfile:\n",
    "    json.dump(pred_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python3 scorer/score.py data/semeval2017-task8-dataset/goldtest/subtaskb.json output/classifier_output/goldtest_nb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
