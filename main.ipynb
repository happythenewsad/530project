{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FileReader import FileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to output..\n"
     ]
    }
   ],
   "source": [
    "classInstance = FileReader\n",
    "df_list = classInstance.get_dataframe() #IMPORTANT:  saves a pickle to output/simple or output/full. \n",
    "\n",
    "#The pickle is the same as df.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT NOTE:\n",
    "\n",
    "Word embeddings are expressed as pickle files.\n",
    "Reading of tweet data is converted to a Pandas Dataframe format and then finally into a pickle file, located output/simple or output/full.\n",
    "\n",
    "It is important that you first execute file_reader.py and *then* this notebook, run_tests.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, \"./feature-extraction/embed-extractor\")\n",
    "from EmbedExtractor import EmbedExtractor\n",
    "sys.path.insert(1, \"./feature-extraction/vulgar-extractor\")\n",
    "from VulgarExtractor import VulgarExtractor\n",
    "sys.path.insert(1, \"./feature-extraction/twitter-parser\")\n",
    "from TwitterParser import TwitterParser\n",
    "sys.path.insert(1, \"./feature-extraction/opinion-extractor/\")\n",
    "from OpinionExtractor import OpinionExtractor\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import classifiers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate the pre-PCA and post-PCA word embeddings\n",
    "\n",
    "###UNCOMMENT THIS TO RUN THE EE#####\n",
    "ee = EmbedExtractor()\n",
    "print(\"EE done.\")\n",
    "tweet2vec = {}\n",
    "    \n",
    "for d in [\"train\",\"dev\",\"test\"]:\n",
    "\twith open('./output/full/'+d+'_data_full.json', 'r') as f:\n",
    "\t    jstr = f.read()\n",
    "\n",
    "\tj = json.loads(jstr)\n",
    "\n",
    "\tfor key in j:\n",
    "\t\ttweet=j[key]['text']\n",
    "\t\ttweet2vec[key]=ee.tweetVec(tweet)\n",
    "pickleFile=\"feature-extraction/embed-extractor/word_embedding_vectors.pickle\"\n",
    "pickle.dump(tweet2vec,open(pickleFile,\"wb\"))\n",
    "# ##########################################\n",
    "\n",
    "\n",
    "word_embeddings_pca = {}\n",
    "pickleFile=\"feature-extraction/embed-extractor/word_embedding_vectors.pickle\"\n",
    "word_embeddings = pd.read_pickle(pickleFile)\n",
    "\n",
    "for word in word_embeddings:\n",
    "    pca = decomposition.PCA(n_components=24)\n",
    "    x = np.array(word_embeddings[word])\n",
    "    try:\n",
    "        x_std = StandardScaler().fit_transform(x)\n",
    "        pca.fit_transform(x_std)\n",
    "        word_embeddings_pca[word]=pca.singular_values_\n",
    "#         print(len(pca.singular_values_))\n",
    "    except ValueError:\n",
    "        # print(word_embeddings_chunked[word])\n",
    "        print(\"UH OH\")\n",
    "\n",
    "pickle.dump(word_embeddings_pca,open(\"feature-extraction/embed-extractor/word_embedding_vectors_pca.pickle\",\"wb\"))\n",
    "for word in word_embeddings_pca:\n",
    "    if len(word_embeddings_pca[word])!=24:\n",
    "        print(\"OH!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opinion_column(df, strongly_subj_list):\n",
    "    #add a binary column where opinion == 1 if the tweet text contains a strongly subjective word\n",
    "    #global strongly_subj_list\n",
    "    OpinionExtractor.add_opinion_column(df, strongly_subj_list)\n",
    "\n",
    "def extract_user_column(row):\n",
    "    global user_labels\n",
    "        \n",
    "    user_dict = row['user']\n",
    "    \n",
    "    for col in user_dict.keys():\n",
    "        user_labels.append(col)\n",
    "\n",
    "def update_user_column(row):\n",
    "    global user_vals\n",
    "    user_dict = row['user']\n",
    "    \n",
    "    for key in user_vals.keys():\n",
    "#         print(key)\n",
    "        concat_key = key[5:]\n",
    "        if concat_key in user_dict:\n",
    "#             print(sup)\n",
    "            val = user_dict[concat_key]\n",
    "            user_vals[key].append(val)\n",
    "        else:\n",
    "            user_vals[key].append(np.nan)\n",
    "            \n",
    "def convert_date(row):\n",
    "    date = row['user_created_at'].split()\n",
    "    date_str = ' '.join([date[1], date[2], date[-1]])\n",
    "    \n",
    "    datetime_object = datetime.strptime(date_str, '%b %d %Y')\n",
    "    date_int = datetime_object.year * 10000 + datetime_object.month * 100 + datetime_object.day\n",
    "        \n",
    "    return date_int\n",
    "\n",
    "def convert_to_int(row, col):\n",
    "    return int(row[col])\n",
    "\n",
    "def normalize_column(df, col):\n",
    "    col_array = np.asarray(df[col].tolist())\n",
    "    mean = np.mean(col_array)\n",
    "    std = np.std(col_array)\n",
    "    col_array = (col_array - mean) / float(std)\n",
    "    \n",
    "    df[col] = col_array\n",
    "    \n",
    "def create_user_features(df):\n",
    "\n",
    "    global user_labels\n",
    "    global user_vals\n",
    "    user_labels = []\n",
    "    df.apply(extract_user_column, axis = 1)\n",
    "    user_labels = ['user_' + label for label in set(user_labels)]\n",
    "    user_vals = {label:[] for label in user_labels}\n",
    "    \n",
    "    df.apply(update_user_column, axis = 1) \n",
    "    \n",
    "    user_df = pd.DataFrame(user_vals)\n",
    "    user_df['user_created'] = user_df.apply(convert_date, axis = 1)\n",
    "\n",
    "    col_list = ['user_default_profile', \n",
    "                'user_favourites_count', 'user_followers_count', 'user_friends_count', 'user_geo_enabled',\n",
    "                'user_listed_count', 'user_statuses_count', 'user_verified','user_created']\n",
    "\n",
    "    for col in col_list:\n",
    "        user_df[col] = user_df.apply(lambda x : convert_to_int(x, col), axis = 1)\n",
    "\n",
    "    # normalize_column(user_df, col)\n",
    "    user_df = user_df[col_list]    \n",
    "\n",
    "    norm_list = ['user_favourites_count', 'user_followers_count', 'user_friends_count',\n",
    "                'user_listed_count', 'user_statuses_count','user_created']\n",
    "\n",
    "    for col in norm_list:\n",
    "        normalize_column(user_df, col)\n",
    "\n",
    "    df = pd.concat([df.reset_index(), user_df], axis = 1)\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feature-extraction/embed-extractor/word_embedding_vectors.pickle', 'rb') as pickle_file:\n",
    "    ee = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in ee.keys():\n",
    "\n",
    "        print(\"key = \" + str(e) + \"   \" + str(len(ee[e])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ee = EmbedExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(column_name, df):\n",
    "    std = df[column_name].std()\n",
    "    norm_col = df[column_name].apply(lambda x: x - std)\n",
    "    df[column_name] = norm_col\n",
    "\n",
    "# builds the labels and vectorizations of given data\n",
    "#if you want to fool around with including/excluding certain features and whatnot, this is the place to do it\n",
    "\n",
    "def labels_and_vectors(file, index=0):\n",
    "    df = pd.read_pickle(file)\n",
    "    \n",
    "    wordlist = VulgarExtractor.vulgarWords(\"feature-extraction/vulgar-extractor/badwords.txt\") \n",
    "    dftext = df[['text']]\n",
    "    result = dftext.applymap(lambda x: VulgarExtractor.containsVulgar(x,wordlist))\n",
    "    df['isVulgar'] = result\n",
    "\n",
    "    word_embeddings = [ee[key] for key in df.index]\n",
    "    # word_embeddings = [ee.tweetVec(tagged_line) for tagged_line in df['text']]\n",
    "    textlist = [txt.replace('\\n','') for txt in df['text'].tolist()]\n",
    "    tagged_sents = TwitterParser.tag(textlist)\n",
    "    df['POS'] = tagged_sents\n",
    "\n",
    "    processed_sents = []\n",
    "    for tagged_sent in df['POS']:\n",
    "        processed_words = []\n",
    "        for word, tag in tagged_sent:\n",
    "            if tag == 'U':\n",
    "                processed_words.append('someurl')\n",
    "            elif tag == '@':\n",
    "                processed_words.append('@someuser')\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "        sent = ' '.join(processed_words)\n",
    "        processed_sents.append(sent)\n",
    "    df['text'] = processed_sents\n",
    "\n",
    "    word_counts = [TwitterParser.word_count(tagged_line) for tagged_line in df['POS']]\n",
    "    pos_count_list = [TwitterParser.pos_counts(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_adjs = [TwitterParser.contains_adjectives(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_urls = [TwitterParser.contains_url(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_emojis = [TwitterParser.contains_emoji(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_abbrevs = [TwitterParser.contains_abbreviation(tagged_line) for tagged_line in df['POS']]\n",
    "\n",
    "    df['wordCount'] = word_counts\n",
    "    df['posCounts'] = pos_count_list\n",
    "    df['containsAdjective'] = contains_adjs\n",
    "    df['containsURL'] = contains_urls\n",
    "    df['containsEmoji'] = contains_emojis\n",
    "    df['containsAbbreviation'] = contains_abbrevs\n",
    "    df['wordEmbedding'] = word_embeddings\n",
    "\n",
    "    \n",
    "    for i, tag in enumerate(TwitterParser.tagset):\n",
    "        tag_counts = []\n",
    "        for pos_counts in df['posCounts']:\n",
    "            tag_counts.append(pos_counts[i])\n",
    "        column_name = 'num_' + tag\n",
    "        df[column_name] = tag_counts\n",
    "        normalize(column_name, df)\n",
    "    \n",
    "#     global strongly_subj_list\n",
    "    strongly_subj_list = OpinionExtractor.initialize_subjectivity()\n",
    "    create_opinion_column(df, strongly_subj_list)\n",
    "    df = create_user_features(df)    \n",
    "        \n",
    "    # Changes \"true\"/\"false\"/\"unverified\" to numeric values, just like the in the early cells\n",
    "    df.loc[df.classification == 'true', 'classification'] = 1\n",
    "    df.loc[df.classification == 'false', 'classification'] = 0\n",
    "    df.loc[df.classification == 'unverified', 'classification'] = 2\n",
    "    # getting the labels\n",
    "\n",
    "    #removed containsURL\n",
    "    attributes = ['isVulgar', 'containsAdjective', 'containsURL', 'containsEmoji', 'containsAbbreviation', 'wordCount']\n",
    "    for tag in TwitterParser.tagset:\n",
    "        attributes.append('num_' + tag)\n",
    "        \n",
    "        \n",
    "    attributes = attributes + ['num_replies', 're_has_?', 're_has_NOT', 're_has_correct',\n",
    " 're_has_credib', 're_has_data', 're_has_detail', 're_has_fabricat', 're_has_lie', 're_has_proof', \n",
    "                  're_has_source', 're_has_witness']\n",
    "                               \n",
    "# 'opinion', 'user_default_profile',\n",
    "#  'user_favourites_count', 'user_followers_count', 'user_friends_count', 'user_geo_enabled', 'user_listed_count', \n",
    "#                   'user_statuses_count', 'user_verified', 'user_created']\n",
    "#     print(df.columns.values)\n",
    "        \n",
    "        \n",
    "    labels = df['classification']\n",
    "    labels = [l for l in labels]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    # getting the values as a list of lists\n",
    "    values = df[attributes].values.tolist()\n",
    "    word_embedding_values = df['wordEmbedding'].values.tolist()\n",
    "\n",
    "\n",
    "#     #Below puts the tweet ID as a feature. Comment this out if you aren't using tweetID\n",
    "#     for i,index in enumerate(df.index):\n",
    "#         dev_values[i].append(int(index))\n",
    "\n",
    "\n",
    "#UNCOMMENT THIS IN ORDER TO INCOPORATE WORD_EMBEDDINGS AGAIN\n",
    "    for i,d in enumerate(word_embedding_values):\n",
    "        values[i].extend(d)\n",
    "\n",
    "    values = np.array(values)\n",
    "    if index == 1:\n",
    "        return df.index, values\n",
    "    \n",
    "    \n",
    "    return labels, values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the indices of labels-to-values should not be mismatched\n",
    "tr_labels, tr_values = labels_and_vectors('output/full/train_data_full.pickle')\n",
    "indices, dev_values = labels_and_vectors('output/full/goldtest_data_full.pickle', index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change classifier here\n",
    "predictions, probabilities = classifiers.random_forest(tr_values, tr_labels, dev_values, 80, 3, \"gini\")\n",
    "ps = []\n",
    "\n",
    "# need to convert the numerical predictions back into their string values\n",
    "for i, p in enumerate(predictions):\n",
    "    if p == 0:\n",
    "        ps.append('false')\n",
    "    if p == 1:\n",
    "        ps.append('true')\n",
    "    if p == 2:\n",
    "        ps.append('unverified')\n",
    "\n",
    "# creates pairings of the prediction and the probability of the prediction\n",
    "pred_probs_pairs = [[ps[i], probabilities[i][predictions[i]]] for i in range(len(predictions))] \n",
    "#attaches the tweetID (called reference_id in the score.py file)\n",
    "pred_dict = {index:pred_probs_pairs[i] for i,index in enumerate(indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './output/classifier_output/'\n",
    "try:\n",
    "    os.stat(output_dir)\n",
    "except:\n",
    "    os.mkdir(output_dir)  \n",
    "\n",
    "with open('output/classifier_output/goldtest_nb.json', 'w') as outfile:\n",
    "    json.dump(pred_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scorer/score.py data/semeval2017-task8-dataset/goldtest/subtaskb.json output/classifier_output/goldtest_nb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
