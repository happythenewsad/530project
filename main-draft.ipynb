{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS530 Final Project - RumorEval Main Notebook\n",
    "## Authors: Yoon Duk Kim, Peter Kong,  Sam Korn, Jason Tang, Noah Weiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to output..\n"
     ]
    }
   ],
   "source": [
    "# import Pandas dataframe\n",
    "from FileReader import FileReader\n",
    "df_list = FileReader.get_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings (if possible move code to separate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(1, \"./feature-extraction/embed-extractor\")\n",
    "# from EmbedExtractor import EmbedExtractor\n",
    "\n",
    "# # Run this cell to generate the pre-PCA and post-PCA word embeddings\n",
    "# print(\"oh\")\n",
    "# def chunkIt(seq, num):\n",
    "#     print(len(seq))\n",
    "#     avg = len(seq) / float(num)\n",
    "#     out = []\n",
    "#     last = 0.0\n",
    "#     zeros = [0]*int(avg)\n",
    "\n",
    "#     while last < len(seq):\n",
    "#         subVector = seq[int(last):int(last + avg)]\n",
    "#         if zeros==subVector and last>4700:\n",
    "#             break\n",
    "#         out.append(subVector)\n",
    "#         last += avg\n",
    "#     return out\n",
    "\n",
    "####UNCOMMENT THIS TO RUN THE EE#####\n",
    "# ee = EmbedExtractor()\n",
    "# tweet2vec = {}\n",
    "    \n",
    "# for d in [\"train\",\"dev\",\"test\"]:\n",
    "# \twith open('./output/full/'+d+'_data_full.json', 'r') as f:\n",
    "# \t    jstr = f.read()\n",
    "\n",
    "# \tj = json.loads(jstr)\n",
    "\n",
    "# \tfor key in j:\n",
    "# \t\ttweet=j[key]['text']\n",
    "# \t\ttweet2vec[key]=ee.tweetVec(tweet)\n",
    "# pickleFile=\"feature-extraction/embed-extractor/word_embedding_vectors.pickle\"\n",
    "# pickle.dump(tweet2vec,open(pickleFile,\"wb\"))\n",
    "# ##########################################\n",
    "\n",
    "\n",
    "# from sklearn import decomposition\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# word_embeddings_pca = {}\n",
    "# pickleFile=\"feature-extraction/embed-extractor/word_embedding_vectors.pickle\"\n",
    "# word_embeddings = pd.read_pickle(pickleFile)\n",
    "\n",
    "# word_embeddings_chunked = {}\n",
    "# for word in word_embeddings:\n",
    "#     word_embeddings_chunked[word]=chunkIt(word_embeddings[word],30)\n",
    "# for word in word_embeddings_chunked:\n",
    "#     pca = decomposition.PCA(n_components=24)\n",
    "#     x = np.array(word_embeddings_chunked[word])\n",
    "#     print(len(x))\n",
    "#     try:\n",
    "#         x_std = StandardScaler().fit_transform(x)\n",
    "#         pca.fit_transform(x_std)\n",
    "#         word_embeddings_pca[word]=pca.singular_values_\n",
    "# #         print(len(pca.singular_values_))\n",
    "#     except ValueError:\n",
    "#         # print(word_embeddings_chunked[word])\n",
    "#         print(\"UH OH\")\n",
    "\n",
    "# pickle.dump(word_embeddings_pca, open(\"feature-extraction/embed-extractor/word_embedding_vectors_pca.pickle\",\"wb\"))\n",
    "# for word in word_embeddings_pca:\n",
    "#     if len(word_embeddings_pca[word])!=16:\n",
    "#         print(\"OH!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion (If possible move code to separate file or elsewhere in code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opinion_column(df, strongly_subj_list):\n",
    "    #add a binary column where opinion == 1 if the tweet text contains a strongly subjective word\n",
    "    #global strongly_subj_list\n",
    "    OpinionExtractor.add_opinion_column(df, strongly_subj_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Features (if possible move code to separate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def extract_user_column(row):\n",
    "    global user_labels\n",
    "        \n",
    "    user_dict = row['user']\n",
    "    \n",
    "    for col in user_dict.keys():\n",
    "        user_labels.append(col)\n",
    "\n",
    "def update_user_column(row):\n",
    "    global user_vals\n",
    "    user_dict = row['user']\n",
    "    \n",
    "    for key in user_vals.keys():\n",
    "        concat_key = key[5:]\n",
    "        if concat_key in user_dict:\n",
    "            val = user_dict[concat_key]\n",
    "            user_vals[key].append(val)\n",
    "        else:\n",
    "            user_vals[key].append(np.nan)\n",
    "            \n",
    "def convert_date(row):\n",
    "    date = row['user_created_at'].split()\n",
    "    date_str = ' '.join([date[1], date[2], date[-1]])\n",
    "    \n",
    "    datetime_object = datetime.strptime(date_str, '%b %d %Y')\n",
    "    date_int = datetime_object.year * 10000 + datetime_object.month * 100 + datetime_object.day\n",
    "        \n",
    "    return date_int\n",
    "\n",
    "def convert_to_int(row, col):\n",
    "    return int(row[col])\n",
    "\n",
    "def normalize_column(df, col):\n",
    "    col_array = np.asarray(df[col].tolist())\n",
    "    mean = np.mean(col_array)\n",
    "    std = np.std(col_array)\n",
    "    col_array = (col_array - mean) / float(std)\n",
    "    \n",
    "    df[col] = col_array\n",
    "    \n",
    "def create_user_features(df):\n",
    "\n",
    "    global user_labels\n",
    "    global user_vals\n",
    "    user_labels = []\n",
    "    df.apply(extract_user_column, axis = 1)\n",
    "    user_labels = ['user_' + label for label in set(user_labels)]\n",
    "    user_vals = {label:[] for label in user_labels}\n",
    "    \n",
    "    df.apply(update_user_column, axis = 1) \n",
    "    \n",
    "    user_df = pd.DataFrame(user_vals)\n",
    "    user_df['user_created'] = user_df.apply(convert_date, axis = 1)\n",
    "\n",
    "    col_list = ['user_default_profile', \n",
    "                'user_favourites_count', 'user_followers_count', 'user_friends_count', 'user_geo_enabled',\n",
    "                'user_listed_count', 'user_statuses_count', 'user_verified','user_created']\n",
    "\n",
    "    for col in col_list:\n",
    "        user_df[col] = user_df.apply(lambda x : convert_to_int(x, col), axis = 1)\n",
    "\n",
    "    # normalize_column(user_df, col)\n",
    "    user_df = user_df[col_list]    \n",
    "\n",
    "    norm_list = ['user_favourites_count', 'user_followers_count', 'user_friends_count',\n",
    "                'user_listed_count', 'user_statuses_count','user_created']\n",
    "\n",
    "    for col in norm_list:\n",
    "        normalize_column(user_df, col)\n",
    "\n",
    "    df = pd.concat([df.reset_index(), user_df], axis = 1)\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More embed stuff... commented out for now. Move to separate file if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('feature-extraction/embed-extractor/word_embedding_vectors.pickle', 'rb') as pickle_file:\n",
    "#     ee = pickle.load(pickle_file)\n",
    "\n",
    "# for e in ee.keys():\n",
    "#     print(\"key = \" + str(e) + \"   \" + str(len(ee[e])))\n",
    "\n",
    "# ee = EmbedExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build labels and vectorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, \"./feature-extraction/vulgar-extractor\")\n",
    "from VulgarExtractor import VulgarExtractor\n",
    "sys.path.insert(1, \"./feature-extraction/twitter-parser\")\n",
    "from TwitterParser import TwitterParser\n",
    "sys.path.insert(1, \"./feature-extraction/opinion-extractor/\")\n",
    "from OpinionExtractor import OpinionExtractor\n",
    "\n",
    "def labels_and_vectors(file, index=0):\n",
    "    # load dataframe\n",
    "    df = pd.read_pickle(file)\n",
    "    \n",
    "\n",
    "    # Get POS tags\n",
    "    textlist = [txt.replace('\\n','') for txt in df['text'].tolist()]\n",
    "    tagged_sents = TwitterParser.tag(textlist)\n",
    "    df['POS'] = tagged_sents\n",
    "\n",
    "\n",
    "    # Preprocess text (maybe eliminate)\n",
    "    processed_sents = []\n",
    "    for tagged_sent in df['POS']:\n",
    "        processed_words = []\n",
    "        for word, tag in tagged_sent:\n",
    "            if tag == 'U':\n",
    "                processed_words.append('someurl')\n",
    "            elif tag == '@':\n",
    "                processed_words.append('@someuser')\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "        sent = ' '.join(processed_words)\n",
    "        processed_sents.append(sent)\n",
    "    df['text'] = processed_sents\n",
    "\n",
    "\n",
    "    # vulgar extractor\n",
    "    wordlist = VulgarExtractor.vulgarWords(\"feature-extraction/vulgar-extractor/badwords.txt\") \n",
    "    dftext = df[['text']]\n",
    "    result = dftext.applymap(lambda x: VulgarExtractor.containsVulgar(x,wordlist))\n",
    "    df['isVulgar'] = result\n",
    "\n",
    "\n",
    "    # word embeddings\n",
    "    # word_embeddings = [ee[key] for key in df.index]\n",
    "    # word_embeddings = [ee.tweetVec(tagged_line) for tagged_line in df['text']]\n",
    "    # df['wordEmbedding'] = word_embeddings\n",
    "\n",
    "\n",
    "    # Twitter text features\n",
    "    word_counts = [TwitterParser.word_count(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_adjs = [TwitterParser.contains_adjectives(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_urls = [TwitterParser.contains_url(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_emojis = [TwitterParser.contains_emoji(tagged_line) for tagged_line in df['POS']]\n",
    "    contains_abbrevs = [TwitterParser.contains_abbreviation(tagged_line) for tagged_line in df['POS']]\n",
    "\n",
    "    df['wordCount'] = word_counts\n",
    "    df['containsAdjective'] = contains_adjs\n",
    "    df['containsURL'] = contains_urls\n",
    "    df['containsEmoji'] = contains_emojis\n",
    "    df['containsAbbreviation'] = contains_abbrevs\n",
    "\n",
    "\n",
    "    # opinion feature\n",
    "    strongly_subj_list = OpinionExtractor.initialize_subjectivity()\n",
    "    create_opinion_column(df, strongly_subj_list)\n",
    "    \n",
    "\n",
    "    # user features\n",
    "    df = create_user_features(df)    \n",
    "      \n",
    "\n",
    "    # attributes\n",
    "    attributes = [\n",
    "        # 'isVulgar',\n",
    "        # 'containsAdjective',\n",
    "        'containsURL',\n",
    "        # 'containsEmoji',\n",
    "        # 'containsAbbreviation',\n",
    "        'wordCount',\n",
    "        # 'opinion',\n",
    "        # 'wordEmbeddings'\n",
    "    ]\n",
    "        \n",
    "    attributes.extend([\n",
    "        'num_replies',\n",
    "        # 're_has_?',\n",
    "        # 're_has_NOT',\n",
    "        're_has_correct',\n",
    "        're_has_credib',\n",
    "        # 're_has_data',\n",
    "        # 're_has_detail',\n",
    "        # 're_has_fabricat',\n",
    "        # 're_has_lie',\n",
    "        're_has_proof',\n",
    "        're_has_source',\n",
    "        're_has_witness',\n",
    "        # 'user_default_profile',\n",
    "        # 'user_favourites_count',\n",
    "        # 'user_followers_count',\n",
    "        # 'user_friends_count',\n",
    "        # 'user_geo_enabled',\n",
    "        # 'user_listed_count',\n",
    "        # 'user_statuses_count',\n",
    "        # 'user_verified',\n",
    "        # 'user_created'\n",
    "    ])\n",
    "\n",
    "    # labels\n",
    "    # Changes \"true\"/\"false\"/\"unverified\" to numeric values, just like the in the early cells\n",
    "    df.loc[df.classification == 'true', 'classification'] = 1\n",
    "    df.loc[df.classification == 'false', 'classification'] = 0\n",
    "    df.loc[df.classification == 'unverified', 'classification'] = 2\n",
    "        \n",
    "    labels = df['classification']\n",
    "    labels = [l for l in labels]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    # getting the values as a list of lists\n",
    "    values = df[attributes].values.tolist()\n",
    "\n",
    "\n",
    "    # UNCOMMENT THIS IN ORDER TO INCOPORATE WORD_EMBEDDINGS AGAIN\n",
    "    # word_embedding_values = df['wordEmbedding'].values.tolist()\n",
    "    # for i,d in enumerate(word_embedding_values):\n",
    "    #     values[i].extend(d)\n",
    "\n",
    "    values = np.array(values)\n",
    "    if index == 1:\n",
    "        return df.index, values\n",
    "    else:\n",
    "        print(attributes)\n",
    "    \n",
    "    return labels, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['containsURL', 'wordCount', 'num_replies', 're_has_correct', 're_has_credib', 're_has_proof', 're_has_source', 're_has_witness']\n"
     ]
    }
   ],
   "source": [
    "tr_labels, tr_values = labels_and_vectors('output/full/train_data_full.pickle')\n",
    "indices, dev_values = labels_and_vectors('output/full/goldtest_data_full.pickle', index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Classifiers\n",
    "\n",
    "predictions, probabilities = Classifiers.naive_bayes(tr_values, tr_labels, dev_values)\n",
    "# predictions, probabilities = classifiers.random_forest(tr_values, tr_labels, dev_values, 80, 3, \"gini\")\n",
    "# predictions, probabilities = classifiers.decision_tree_classifier(tr_values, tr_labels, dev_values\")\n",
    "# predictions, probabilities = classifiers.svm_classifier(tr_values, tr_labels, dev_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numerical predictions back into their string values\n",
    "ps = []\n",
    "for i, p in enumerate(predictions):\n",
    "    if p == 0:\n",
    "        ps.append('false')\n",
    "    if p == 1:\n",
    "        ps.append('true')\n",
    "    if p == 2:\n",
    "        ps.append('unverified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates pairings of the prediction and the probability of the prediction\n",
    "pred_probs_pairs = [[ps[i], probabilities[i][predictions[i]]] for i in range(len(predictions))] \n",
    "\n",
    "# attaches the tweetID (called reference_id in the score.py file)\n",
    "pred_dict = {index:pred_probs_pairs[i] for i,index in enumerate(indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes the output\n",
    "output_dir = './output/'\n",
    "try:\n",
    "    os.stat(output_dir)\n",
    "except:\n",
    "    os.mkdir(output_dir)  \n",
    "\n",
    "with open('output/test_predictions.json', 'w') as outfile:\n",
    "    json.dump(pred_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
