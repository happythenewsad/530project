{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN classifier for rumoreval that solely uses the text attribute of the tweets for classification. Similar to homework 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(1, \"./code/rnn_text\")\n",
    "import train\n",
    "import model\n",
    "import helpers\n",
    "import sys\n",
    "sys.path.insert(1, \"./code/feature-extraction/embed-extractor\")\n",
    "sys.path.insert(1, \"./code/feature-extraction/vulgar-extractor\")\n",
    "from VulgarExtractor import VulgarExtractor\n",
    "sys.path.insert(1, \"./code/feature-extraction/twitter-parser\")\n",
    "from TwitterParser import TwitterParser\n",
    "import classifiers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import helpers\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only attribute here is the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our parameters. RNN initialization extremely similar to the one used in the PyTorch RNN tutorial for classifying baby names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "n_epochs = 2000\n",
    "hidden_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "decoder = model.RNN(len(helpers.all_letters), hidden_size=hidden_size, output_size =3)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here we can define functions that normalize and transform our data suitable for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need stuff from main.py to test transformation into Tensors\n",
    "\n",
    "def normalize(column_name, df):\n",
    "    std = df[column_name].std()\n",
    "    norm_col = df[column_name].apply(lambda x: x - std)\n",
    "    df[column_name] = norm_col\n",
    "\n",
    "# builds the labels and vectorizations of given data\n",
    "#if you want to fool around with including/excluding certain features and whatnot, this is the place to do it\n",
    "\n",
    "def labels_and_vectors(file, index=0):\n",
    "    df = pd.read_pickle(file)\n",
    "    \n",
    "    wordlist = VulgarExtractor.vulgarWords(\"code/feature-extraction/vulgar-extractor/badwords.txt\") \n",
    "    dftext = df[['text']]\n",
    "    result = dftext.applymap(lambda x: VulgarExtractor.containsVulgar(x,wordlist))\n",
    "    df['isVulgar'] = result\n",
    "\n",
    "    #word_embeddings = [ee.tweetVec(tagged_line) for tagged_line in df['text']]\n",
    "    textlist = [txt.replace('\\n','') for txt in df['text'].tolist()]\n",
    "    tagged_sents = TwitterParser.tag(textlist)\n",
    "    df['POS'] = tagged_sents\n",
    "\n",
    "    processed_sents = []\n",
    "    for tagged_sent in df['POS']:\n",
    "        processed_words = []\n",
    "        for word, tag in tagged_sent:\n",
    "            if tag == 'U':\n",
    "                processed_words.append('someurl')\n",
    "            elif tag == '@':\n",
    "                processed_words.append('@someuser')\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "        sent = ' '.join(processed_words)\n",
    "        processed_sents.append(sent)\n",
    "    df['text'] = processed_sents\n",
    "        \n",
    "    # Changes \"true\"/\"false\"/\"unverified\" to numeric values, just like the in the early cells\n",
    "    df.loc[df.classification == 'true', 'classification'] = 1\n",
    "    df.loc[df.classification == 'false', 'classification'] = 2\n",
    "    df.loc[df.classification == 'unverified', 'classification'] = 0\n",
    "    \n",
    "    # getting the labels\n",
    "    labels = df['classification']\n",
    "    labels = [l for l in labels]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    # getting the values as a list of lists\n",
    "    values = df[attributes].values.tolist()\n",
    "\n",
    "    \n",
    "    values = np.array(values)\n",
    "    if index == 1:\n",
    "        return df.index, values\n",
    "\n",
    "    return labels, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here, you can set where your training and target data comes from (we just \"call it\" dev_values here). Please maintain the index variable seen in the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './feature-extraction/twitter-parser/tweets.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7178d575e78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_and_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/simple/train_data_simple.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlabels_and_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/simple/goldtest_data_simple.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-281207ad7b93>\u001b[0m in \u001b[0;36mlabels_and_vectors\u001b[0;34m(file, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#word_embeddings = [ee.tweetVec(tagged_line) for tagged_line in df['text']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtextlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagged_sents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m./feature-extraction/twitter-parser/TwitterParser.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(tweets)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './feature-extraction/twitter-parser/tweets.txt'"
     ]
    }
   ],
   "source": [
    "tr_labels, tr_values = labels_and_vectors('output/simple/train_data_simple.pickle')\n",
    "indices, dev_values,= labels_and_vectors('output/simple/goldtest_data_simple.pickle', index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further transformation of data such that we can make it suitable for the torch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = [arr.tolist() for arr in tr_values]\n",
    "dvs = [arr.tolist() for arr in dev_values]\n",
    "tls = [l.item() for l in tr_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our parameters. RNN initialization extremely similar to the one used in the PyTorch RNN tutorial for classifying baby names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "n_epochs = 2000\n",
    "hidden_size = 20\n",
    "learning_rate = 0.001\n",
    "decoder = model.RNN(len(helpers.all_letters), hidden_size=hidden_size, output_size =3)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the pytorch tutorial for RNNs, this is a function that goes through the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all():\n",
    "    for i, label in enumerate(tls):\n",
    "        tensor = helpers.line_to_tensor(tvs[i][0])  \n",
    "        category_tensor = Variable(torch.LongTensor([label]))\n",
    "        line_tensor = Variable(tensor)\n",
    "        output, loss = train.train(category_tensor, line_tensor,decoder)\n",
    "        decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "By executing this script, we train the model get the label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    train_all()\n",
    "    \n",
    "predictions = []\n",
    "for d in dvs:\n",
    "    tensor = helpers.line_to_tensor(d)  \n",
    "    line_tensor = Variable(tensor)\n",
    "    output = decoder.predict(line_tensor)\n",
    "    predictions.append(output)\n",
    "\n",
    "    \n",
    "    \n",
    "ps = []\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    p = p[0][0]\n",
    "    if p == 2:\n",
    "        ps.append('false')\n",
    "    if p == 1:\n",
    "        ps.append('true')\n",
    "    if p == 0:\n",
    "        ps.append('unverified')\n",
    "\n",
    "pred_dict = {index:(ps[i],1) for i,index in enumerate(indices)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to json and scoring script. The first argument is the gold set and the second argument is the predictions you should have generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/rnn/goldtest_rnn_text.json', 'w') as outfile:\n",
    "    json.dump(pred_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scorer/score.py data/semeval2017-task8-dataset/goldtest/subtaskb.json output/rnn/goldtest_rnn_text.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
